{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30b02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "import pytesseract\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc006b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a6364",
   "metadata": {},
   "source": [
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b5a8986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac35bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_components():\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"all-MiniLM-L6-v2\",\n",
    "            cache_folder=\"./model_cache\",  # Specify a local cache directory\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        return embeddings, text_splitter\n",
    "    except OSError as e:\n",
    "        st.error(f\"Error initializing components: {str(e)}\")\n",
    "        st.info(\"Please check your internet connection and try again.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc5437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_file(uploaded_file):\n",
    "    text = \"\"\n",
    "    file_ext = uploaded_file.name.split('.')[-1].lower()\n",
    "    \n",
    "    try:\n",
    "        if file_ext == 'pdf':\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:\n",
    "                temp_file.write(uploaded_file.getvalue())\n",
    "                temp_path = temp_file.name\n",
    "            \n",
    "            pdf_reader = PdfReader(temp_path)\n",
    "            text = \"\\n\".join([page.extract_text() or \"\" for page in pdf_reader.pages])\n",
    "            os.unlink(temp_path)  # Clean up temp file\n",
    "            \n",
    "        elif file_ext == 'docx':\n",
    "            doc = Document(io.BytesIO(uploaded_file.getvalue()))\n",
    "            text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "            \n",
    "        elif file_ext in ['png', 'jpg', 'jpeg']:\n",
    "            image = Image.open(uploaded_file)\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            \n",
    "        elif file_ext == 'txt':\n",
    "            text = str(uploaded_file.getvalue(), \"utf-8\")\n",
    "            \n",
    "        elif file_ext in ['xlsx', 'xls', 'csv']:\n",
    "            if file_ext == 'csv':\n",
    "                df = pd.read_csv(uploaded_file)\n",
    "            else:\n",
    "                df = pd.read_excel(uploaded_file)\n",
    "            \n",
    "            # Store dataframe in session state for direct access\n",
    "            st.session_state.excel_data = df\n",
    "            \n",
    "            # Convert dataframe to text for RAG processing\n",
    "            text = df.to_string(index=False)\n",
    "            # Add column names and data types as metadata to improve RAG\n",
    "            text += \"\\n\\nTable Structure:\\n\"\n",
    "            text += f\"Columns: {', '.join(df.columns.tolist())}\\n\"\n",
    "            text += f\"Number of records: {len(df)}\\n\"\n",
    "            for col in df.columns:\n",
    "                text += f\"Column '{col}' has data type: {df[col].dtype}\\n\"\n",
    "                if df[col].dtype in ['object', 'string']:\n",
    "                    sample_values = df[col].dropna().unique()[:5]\n",
    "                    if len(sample_values) > 0:\n",
    "                        text += f\"Sample values for '{col}': {', '.join(str(x) for x in sample_values)}\\n\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        st.error(f\"Error processing file: {str(e)}\")\n",
    "        return None\n",
    "    return text if text.strip() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c050be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(uploaded_file, text_splitter, embeddings):\n",
    "    text = extract_text_from_file(uploaded_file)\n",
    "    if not text:\n",
    "        st.error(\"No readable text found in the document\")\n",
    "        return None\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    if not chunks:\n",
    "        st.error(\"Document processing resulted in zero chunks\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error creating vector store: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01132537",
   "metadata": {},
   "source": [
    "Summarization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934addaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(vectorstore, llm):\n",
    "    # Get all documents from vector store\n",
    "    docs = vectorstore.similarity_search(\"\", k=50)  # empty query â†’ retrieve many chunks\n",
    "\n",
    "    # Join all chunks into one text\n",
    "    all_text = \" \".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # Run summarization chain\n",
    "    summarization_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=get_summarization_prompt()\n",
    "    )\n",
    "\n",
    "    summary = summarization_chain.run(context=all_text)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f08275",
   "metadata": {},
   "source": [
    "Summarization prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da3856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarization_prompt():\n",
    "    return PromptTemplate.from_template(\"\"\"\n",
    "    Summarize the following text in clear, concise language...\n",
    "    Text:\n",
    "    {context}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250bb0d8",
   "metadata": {},
   "source": [
    "Custom Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246d07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_custom_prompt():\n",
    "    return PromptTemplate(\n",
    "        template=\"\"\"Use the following pieces of context to answer the question at the end. \n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer in detail and be specific. If relevant, include page numbers or sections from the document:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db83240",
   "metadata": {},
   "source": [
    "# STREAMLIT UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a4750b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 18:04:40.141 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "c:\\Users\\vansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 220\u001b[39m\n\u001b[32m    217\u001b[39m         st.warning(\u001b[33m\"\u001b[39m\u001b[33mDocument processing failed. Please check if the document contains readable text.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      2\u001b[39m     st.set_page_config(\n\u001b[32m      3\u001b[39m         page_title=\u001b[33m\"\u001b[39m\u001b[33mAdvanced Document Analysis with Groq\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m         page_icon=\u001b[33m\"\u001b[39m\u001b[33mðŸ§ \u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m         layout=\u001b[33m\"\u001b[39m\u001b[33mwide\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     components = \u001b[43minitialize_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     10\u001b[39m         st.stop()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36minitialize_components\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitialize_components\u001b[39m():\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         embeddings = \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./model_cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify a local cache directory\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnormalize_embeddings\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m         text_splitter = RecursiveCharacterTextSplitter(\n\u001b[32m      9\u001b[39m             chunk_size=\u001b[32m1000\u001b[39m,\n\u001b[32m     10\u001b[39m             chunk_overlap=\u001b[32m200\u001b[39m,\n\u001b[32m     11\u001b[39m             length_function=\u001b[38;5;28mlen\u001b[39m\n\u001b[32m     12\u001b[39m         )\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m embeddings, text_splitter\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:69\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(**kwargs)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     71\u001b[39m     msg = (\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m2.2.2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m __MODEL_HUB_ORGANIZATION__ = \u001b[33m'\u001b[39m\u001b[33msentence-transformers\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceTransformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mDenoisingAutoEncoderDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mNoDuplicatesDataLoader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mParallelSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceLabelDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\datasets\\ParallelSentencesDataset.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ndarray\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi, HfFolder, Repository, hf_hub_url, cached_download\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\__init__.py:950\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m    948\u001b[39m _import_structure = {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure.items()}\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m import_structure = \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[34;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})].update(_import_structure)\n\u001b[32m    953\u001b[39m sys.modules[\u001b[34m__name__\u001b[39m] = _LazyModule(\n\u001b[32m    954\u001b[39m     \u001b[34m__name__\u001b[39m,\n\u001b[32m    955\u001b[39m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m\"\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m     extra_objects={\u001b[33m\"\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m\"\u001b[39m: __version__},\n\u001b[32m    959\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2841\u001b[39m, in \u001b[36mdefine_import_structure\u001b[39m\u001b[34m(module_path, prefix)\u001b[39m\n\u001b[32m   2817\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[32m   2818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> IMPORT_STRUCTURE_T:\n\u001b[32m   2819\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2820\u001b[39m \u001b[33;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[32m   2821\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2839\u001b[39m \u001b[33;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[32m   2840\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2841\u001b[39m     import_structure = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2842\u001b[39m     spread_dict = spread_import_structure(import_structure)\n\u001b[32m   2844\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2554\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(module_path):\n\u001b[32m   2553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f != \u001b[33m\"\u001b[39m\u001b[33m__pycache__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os.path.isdir(os.path.join(module_path, f)):\n\u001b[32m-> \u001b[39m\u001b[32m2554\u001b[39m         import_structure[f] = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(os.path.join(directory, f)):\n\u001b[32m   2557\u001b[39m         adjacent_modules.append(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vansh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2578\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name.endswith(\u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2576\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   2579\u001b[39m     file_content = f.read()\n\u001b[32m   2581\u001b[39m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:309\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    st.set_page_config(\n",
    "        page_title=\"Advanced Document Analysis with Groq\",\n",
    "        page_icon=\"ðŸ§ \",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    \n",
    "    components = initialize_components()\n",
    "    if components is None:\n",
    "        st.stop()\n",
    "    \n",
    "    embeddings, text_splitter = components\n",
    "    \n",
    "    # Check for Tesseract OCR quietly\n",
    "    tesseract_available = True\n",
    "    try:\n",
    "        version = pytesseract.get_tesseract_version()\n",
    "        if not version:\n",
    "            tesseract_available = False\n",
    "    except Exception:\n",
    "        tesseract_available = False\n",
    "    \n",
    "    st.title(\"ðŸ§  Advanced Document Analysis with Groq\")\n",
    "    st.write(\"Upload any document and get AI-powered insights\")\n",
    "    \n",
    "    # Initialize session state\n",
    "    if 'vector_store' not in st.session_state:\n",
    "        st.session_state.vector_store = None\n",
    "    if 'processed_file' not in st.session_state:\n",
    "        st.session_state.processed_file = None\n",
    "    if 'excel_data' not in st.session_state:\n",
    "        st.session_state.excel_data = None\n",
    "    \n",
    "    # Sidebar configuration\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configuration\")\n",
    "        groq_api_key = st.text_input(\n",
    "            \"Groq API Key\",\n",
    "            type=\"password\",\n",
    "            value=os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "        )\n",
    "        \n",
    "        if groq_api_key:\n",
    "            os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
    "        else:\n",
    "            st.warning(\"Please enter your Groq API Key to generate answers\")\n",
    "        \n",
    "        model_name = st.selectbox(\n",
    "            \"Select Groq Model\",\n",
    "            [\"llama3-70b-8192\", \"llama3-8b-8192\", \"mixtral-8x7b-4096\", \"gemma-7b-it\"],\n",
    "            index=0\n",
    "        )\n",
    "        \n",
    "        temperature = st.slider(\n",
    "            \"Creativity (Temperature)\",\n",
    "            min_value=0.0,\n",
    "            max_value=1.0,\n",
    "            value=0.3,\n",
    "            step=0.1\n",
    "        )\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"### Supported Formats\")\n",
    "        st.markdown(\"- PDF, DOCX, TXT, PNG, JPG\")\n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"### How to Use\")\n",
    "        st.markdown(\"1. Upload a document\\n2. Ask questions\\n3. Get detailed answers\")\n",
    "    \n",
    "    # File upload section\n",
    "    uploaded_file = st.file_uploader(\n",
    "        \"Choose a document\",\n",
    "        type=[\"pdf\", \"docx\", \"txt\", \"png\", \"jpg\", \"jpeg\", \"xlsx\", \"xls\", \"csv\"],\n",
    "        accept_multiple_files=False,\n",
    "        key=\"file_uploader\"\n",
    "    )\n",
    "    \n",
    "    # Show Tesseract warning only when image file is uploaded\n",
    "    if uploaded_file and not tesseract_available and uploaded_file.name.split('.')[-1].lower() in ['png', 'jpg', 'jpeg']:\n",
    "        st.info(\"ðŸ“Œ Note: Tesseract OCR is not detected. Text extraction from images may not work properly. For best results with images, consider installing Tesseract OCR.\", icon=\"â„¹ï¸\")\n",
    "    \n",
    "    # Process document if new file uploaded\n",
    "    if uploaded_file and (st.session_state.processed_file != uploaded_file.name):\n",
    "        with st.spinner(\"Processing document...\"):\n",
    "            vector_store = process_document(uploaded_file, text_splitter, embeddings)\n",
    "            \n",
    "            if vector_store:\n",
    "                st.session_state.vector_store = vector_store\n",
    "                st.session_state.processed_file = uploaded_file.name\n",
    "                st.success(f\"Document processed successfully! ({len(vector_store.index_to_docstore_id)} chunks)\")\n",
    "            else:\n",
    "                st.error(\"Failed to process document. Please try a different file.\")\n",
    "    \n",
    "    # Question and answer section\n",
    "    if st.session_state.vector_store:\n",
    "        st.subheader(\"Ask About the Document\")\n",
    "        \n",
    "        # Excel data preview if available\n",
    "        file_ext = \"\"\n",
    "        if uploaded_file:\n",
    "            file_ext = uploaded_file.name.split('.')[-1].lower()\n",
    "        \n",
    "        if file_ext in ['xlsx', 'xls', 'csv'] and st.session_state.excel_data is not None:\n",
    "            with st.expander(\"Preview Data\", expanded=True):\n",
    "                st.dataframe(st.session_state.excel_data, use_container_width=True)\n",
    "                \n",
    "                # Quick filter for Excel data\n",
    "                if not st.session_state.excel_data.empty:\n",
    "                    st.subheader(\"Quick Filter\")\n",
    "                    \n",
    "                    # Select column to filter\n",
    "                    cols = st.session_state.excel_data.columns.tolist()\n",
    "                    filter_col = st.selectbox(\"Select column to filter\", cols)\n",
    "                    \n",
    "                    # Get unique values for the selected column\n",
    "                    unique_values = st.session_state.excel_data[filter_col].dropna().unique().tolist()\n",
    "                    if len(unique_values) > 0:\n",
    "                        selected_value = st.selectbox(f\"Select value from {filter_col}\", unique_values)\n",
    "                        \n",
    "                        # Filter data\n",
    "                        filtered_data = st.session_state.excel_data[st.session_state.excel_data[filter_col] == selected_value]\n",
    "                        st.write(f\"Filtered data for {filter_col} = {selected_value}:\")\n",
    "                        st.dataframe(filtered_data, use_container_width=True)\n",
    "                        \n",
    "                        # Quick questions based on filtered data\n",
    "                        employee_suggestion = f\"Show me details about {selected_value}\" if filter_col == \"Name\" else \"\"\n",
    "                        summary_suggestion = f\"Summarize information about {selected_value}\"\n",
    "                        st.write(\"Quick questions:\")\n",
    "                        quick_q_col1, quick_q_col2 = st.columns(2)\n",
    "                        \n",
    "                        with quick_q_col1:\n",
    "                            if st.button(employee_suggestion or \"Show details\"):\n",
    "                                st.session_state.question = employee_suggestion or f\"Show all details for {filter_col} = {selected_value}\"\n",
    "                        \n",
    "                        with quick_q_col2:\n",
    "                            if st.button(summary_suggestion):\n",
    "                                st.session_state.question = summary_suggestion\n",
    "                        \n",
    "        # Initialize question state if not exist\n",
    "        if 'question' not in st.session_state:\n",
    "            st.session_state.question = \"\"\n",
    "            \n",
    "        question = st.text_area(\n",
    "            \"Enter your question\",\n",
    "            value=st.session_state.question,\n",
    "            placeholder=\"What is the main point of this document? Or for employee data: 'Show me details about John Smith'\",\n",
    "            height=100\n",
    "        )\n",
    "        \n",
    "        # Store question in session state\n",
    "        st.session_state.question = question\n",
    "        \n",
    "        col1, col2 = st.columns([1, 3])\n",
    "        with col1:\n",
    "            submit_button = st.button(\"Get Answer\", use_container_width=True)\n",
    "        \n",
    "        if submit_button and question:\n",
    "            if not groq_api_key:\n",
    "                st.error(\"Please enter your Groq API Key in the sidebar\")\n",
    "            else:\n",
    "                with st.spinner(\"Generating answer...\"):\n",
    "                    try:\n",
    "                        llm = ChatGroq(\n",
    "                            model_name=model_name,\n",
    "                            temperature=temperature,\n",
    "                            api_key=groq_api_key\n",
    "                        )\n",
    "                        \n",
    "                        retriever = st.session_state.vector_store.as_retriever(\n",
    "                            search_kwargs={\"k\": 4}  # Retrieve top 4 most relevant chunks\n",
    "                        )\n",
    "                        \n",
    "                        qa_chain = RetrievalQA.from_chain_type(\n",
    "                            llm=llm,\n",
    "                            chain_type=\"stuff\",\n",
    "                            retriever=retriever,\n",
    "                            chain_type_kwargs={\"prompt\": get_custom_prompt()},\n",
    "                            return_source_documents=True\n",
    "                        )\n",
    "                        \n",
    "                        result = qa_chain.invoke({\"query\": question})\n",
    "                        \n",
    "                        st.subheader(\"Answer\")\n",
    "                        st.markdown(result[\"result\"])\n",
    "                        \n",
    "                        with st.expander(\"View Source Documents\"):\n",
    "                            for i, doc in enumerate(result[\"source_documents\"]):\n",
    "                                st.markdown(f\"**Source {i+1}**\")\n",
    "                                st.text(doc.page_content)\n",
    "                                st.markdown(\"---\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        st.error(f\"Error generating answer: {str(e)}\")\n",
    "                        st.info(\"Make sure your Groq API Key is valid and the service is available\")\n",
    "        # --- Summarization Section ---\n",
    "        st.subheader(\"Summarize the Document\")\n",
    "        if st.button(\"Summarize Document\"):\n",
    "            if not groq_api_key:\n",
    "                st.error(\"Please enter your Groq API Key in the sidebar\")\n",
    "            else:\n",
    "                with st.spinner(\"Generating summary...\"):\n",
    "                    try:\n",
    "                        llm = ChatGroq(\n",
    "                            model_name=model_name,\n",
    "                            temperature=temperature,\n",
    "                            api_key=groq_api_key)\n",
    "                        docs = [doc.page_content for doc in st.session_state.vector_store.docstore._dict.values()]\n",
    "                        summarize_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "                        summary = summarize_chain.run(docs)\n",
    "\n",
    "                        st.subheader(\"Summary\")\n",
    "                        st.success(summary)\n",
    "                    except Exception as e:\n",
    "                        st.error(f\"Error generating summary: {str(e)}\")\n",
    "\n",
    "    \n",
    "    elif uploaded_file and not st.session_state.vector_store:\n",
    "        st.warning(\"Document processing failed. Please check if the document contains readable text.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
